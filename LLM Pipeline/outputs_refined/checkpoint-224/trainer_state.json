{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.0,
  "eval_steps": 500,
  "global_step": 224,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.018018018018018018,
      "grad_norm": 0.35659685730934143,
      "learning_rate": 0.0,
      "loss": 2.2972,
      "step": 1
    },
    {
      "epoch": 0.036036036036036036,
      "grad_norm": 0.31677964329719543,
      "learning_rate": 1e-05,
      "loss": 2.2253,
      "step": 2
    },
    {
      "epoch": 0.05405405405405406,
      "grad_norm": 0.38920214772224426,
      "learning_rate": 2e-05,
      "loss": 2.2894,
      "step": 3
    },
    {
      "epoch": 0.07207207207207207,
      "grad_norm": 0.3193337321281433,
      "learning_rate": 3e-05,
      "loss": 2.2963,
      "step": 4
    },
    {
      "epoch": 0.09009009009009009,
      "grad_norm": 0.3997240662574768,
      "learning_rate": 4e-05,
      "loss": 2.2599,
      "step": 5
    },
    {
      "epoch": 0.10810810810810811,
      "grad_norm": 0.342878133058548,
      "learning_rate": 5e-05,
      "loss": 2.2508,
      "step": 6
    },
    {
      "epoch": 0.12612612612612611,
      "grad_norm": 0.3474276065826416,
      "learning_rate": 6e-05,
      "loss": 2.3674,
      "step": 7
    },
    {
      "epoch": 0.14414414414414414,
      "grad_norm": 0.3681899607181549,
      "learning_rate": 7e-05,
      "loss": 2.2843,
      "step": 8
    },
    {
      "epoch": 0.16216216216216217,
      "grad_norm": 0.3365919291973114,
      "learning_rate": 8e-05,
      "loss": 2.2371,
      "step": 9
    },
    {
      "epoch": 0.18018018018018017,
      "grad_norm": 0.3141583800315857,
      "learning_rate": 9e-05,
      "loss": 2.3222,
      "step": 10
    },
    {
      "epoch": 0.1981981981981982,
      "grad_norm": 0.3146788775920868,
      "learning_rate": 0.0001,
      "loss": 2.2953,
      "step": 11
    },
    {
      "epoch": 0.21621621621621623,
      "grad_norm": 0.3033735156059265,
      "learning_rate": 9.953271028037385e-05,
      "loss": 2.2262,
      "step": 12
    },
    {
      "epoch": 0.23423423423423423,
      "grad_norm": 0.36259564757347107,
      "learning_rate": 9.906542056074767e-05,
      "loss": 2.1486,
      "step": 13
    },
    {
      "epoch": 0.25225225225225223,
      "grad_norm": 0.3685246407985687,
      "learning_rate": 9.85981308411215e-05,
      "loss": 2.3601,
      "step": 14
    },
    {
      "epoch": 0.2702702702702703,
      "grad_norm": 0.3443733751773834,
      "learning_rate": 9.813084112149533e-05,
      "loss": 2.2557,
      "step": 15
    },
    {
      "epoch": 0.2882882882882883,
      "grad_norm": 0.3765426278114319,
      "learning_rate": 9.766355140186917e-05,
      "loss": 2.2548,
      "step": 16
    },
    {
      "epoch": 0.3063063063063063,
      "grad_norm": 0.33229801058769226,
      "learning_rate": 9.7196261682243e-05,
      "loss": 2.3008,
      "step": 17
    },
    {
      "epoch": 0.32432432432432434,
      "grad_norm": 0.3857229948043823,
      "learning_rate": 9.672897196261684e-05,
      "loss": 2.2587,
      "step": 18
    },
    {
      "epoch": 0.34234234234234234,
      "grad_norm": 0.2942904531955719,
      "learning_rate": 9.626168224299066e-05,
      "loss": 2.1531,
      "step": 19
    },
    {
      "epoch": 0.36036036036036034,
      "grad_norm": 0.37403562664985657,
      "learning_rate": 9.579439252336449e-05,
      "loss": 2.1562,
      "step": 20
    },
    {
      "epoch": 0.3783783783783784,
      "grad_norm": 0.3117464482784271,
      "learning_rate": 9.532710280373832e-05,
      "loss": 2.0873,
      "step": 21
    },
    {
      "epoch": 0.3963963963963964,
      "grad_norm": 0.33945953845977783,
      "learning_rate": 9.485981308411216e-05,
      "loss": 2.2082,
      "step": 22
    },
    {
      "epoch": 0.4144144144144144,
      "grad_norm": 0.4501790404319763,
      "learning_rate": 9.439252336448599e-05,
      "loss": 2.2051,
      "step": 23
    },
    {
      "epoch": 0.43243243243243246,
      "grad_norm": 0.3568320572376251,
      "learning_rate": 9.392523364485983e-05,
      "loss": 2.0445,
      "step": 24
    },
    {
      "epoch": 0.45045045045045046,
      "grad_norm": 0.45279550552368164,
      "learning_rate": 9.345794392523365e-05,
      "loss": 2.1496,
      "step": 25
    },
    {
      "epoch": 0.46846846846846846,
      "grad_norm": 0.3942102789878845,
      "learning_rate": 9.299065420560748e-05,
      "loss": 2.091,
      "step": 26
    },
    {
      "epoch": 0.4864864864864865,
      "grad_norm": 0.3600637912750244,
      "learning_rate": 9.252336448598131e-05,
      "loss": 2.0993,
      "step": 27
    },
    {
      "epoch": 0.5045045045045045,
      "grad_norm": 0.3741814196109772,
      "learning_rate": 9.205607476635515e-05,
      "loss": 2.1241,
      "step": 28
    },
    {
      "epoch": 0.5225225225225225,
      "grad_norm": 0.4025864005088806,
      "learning_rate": 9.158878504672898e-05,
      "loss": 2.016,
      "step": 29
    },
    {
      "epoch": 0.5405405405405406,
      "grad_norm": 0.4238741099834442,
      "learning_rate": 9.112149532710282e-05,
      "loss": 2.1155,
      "step": 30
    },
    {
      "epoch": 0.5585585585585585,
      "grad_norm": 0.3685082197189331,
      "learning_rate": 9.065420560747664e-05,
      "loss": 2.1488,
      "step": 31
    },
    {
      "epoch": 0.5765765765765766,
      "grad_norm": 0.352394163608551,
      "learning_rate": 9.018691588785047e-05,
      "loss": 2.0357,
      "step": 32
    },
    {
      "epoch": 0.5945945945945946,
      "grad_norm": 0.3091467618942261,
      "learning_rate": 8.97196261682243e-05,
      "loss": 2.1347,
      "step": 33
    },
    {
      "epoch": 0.6126126126126126,
      "grad_norm": 0.3771456778049469,
      "learning_rate": 8.925233644859814e-05,
      "loss": 2.1022,
      "step": 34
    },
    {
      "epoch": 0.6306306306306306,
      "grad_norm": 0.3556995391845703,
      "learning_rate": 8.878504672897197e-05,
      "loss": 2.0418,
      "step": 35
    },
    {
      "epoch": 0.6486486486486487,
      "grad_norm": 0.37073004245758057,
      "learning_rate": 8.831775700934581e-05,
      "loss": 2.0367,
      "step": 36
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.30255404114723206,
      "learning_rate": 8.785046728971964e-05,
      "loss": 2.2434,
      "step": 37
    },
    {
      "epoch": 0.6846846846846847,
      "grad_norm": 0.3284091651439667,
      "learning_rate": 8.738317757009346e-05,
      "loss": 2.113,
      "step": 38
    },
    {
      "epoch": 0.7027027027027027,
      "grad_norm": 0.34814751148223877,
      "learning_rate": 8.691588785046729e-05,
      "loss": 1.9676,
      "step": 39
    },
    {
      "epoch": 0.7207207207207207,
      "grad_norm": 0.34844255447387695,
      "learning_rate": 8.644859813084113e-05,
      "loss": 2.0203,
      "step": 40
    },
    {
      "epoch": 0.7387387387387387,
      "grad_norm": 0.3490302264690399,
      "learning_rate": 8.598130841121496e-05,
      "loss": 2.1411,
      "step": 41
    },
    {
      "epoch": 0.7567567567567568,
      "grad_norm": 0.41503092646598816,
      "learning_rate": 8.55140186915888e-05,
      "loss": 1.9315,
      "step": 42
    },
    {
      "epoch": 0.7747747747747747,
      "grad_norm": 0.3562829792499542,
      "learning_rate": 8.504672897196261e-05,
      "loss": 1.9153,
      "step": 43
    },
    {
      "epoch": 0.7927927927927928,
      "grad_norm": 0.3314894437789917,
      "learning_rate": 8.457943925233645e-05,
      "loss": 1.8859,
      "step": 44
    },
    {
      "epoch": 0.8108108108108109,
      "grad_norm": 0.3667236268520355,
      "learning_rate": 8.411214953271028e-05,
      "loss": 2.0485,
      "step": 45
    },
    {
      "epoch": 0.8288288288288288,
      "grad_norm": 0.35151124000549316,
      "learning_rate": 8.364485981308412e-05,
      "loss": 2.031,
      "step": 46
    },
    {
      "epoch": 0.8468468468468469,
      "grad_norm": 0.30784955620765686,
      "learning_rate": 8.317757009345795e-05,
      "loss": 2.0545,
      "step": 47
    },
    {
      "epoch": 0.8648648648648649,
      "grad_norm": 0.3332081437110901,
      "learning_rate": 8.271028037383179e-05,
      "loss": 2.2031,
      "step": 48
    },
    {
      "epoch": 0.8828828828828829,
      "grad_norm": 0.33717548847198486,
      "learning_rate": 8.22429906542056e-05,
      "loss": 2.0496,
      "step": 49
    },
    {
      "epoch": 0.9009009009009009,
      "grad_norm": 0.3565555214881897,
      "learning_rate": 8.177570093457944e-05,
      "loss": 1.9668,
      "step": 50
    },
    {
      "epoch": 0.918918918918919,
      "grad_norm": 0.3324648439884186,
      "learning_rate": 8.130841121495327e-05,
      "loss": 2.1356,
      "step": 51
    },
    {
      "epoch": 0.9369369369369369,
      "grad_norm": 0.3309403955936432,
      "learning_rate": 8.084112149532711e-05,
      "loss": 2.2059,
      "step": 52
    },
    {
      "epoch": 0.954954954954955,
      "grad_norm": 0.37627509236335754,
      "learning_rate": 8.037383177570094e-05,
      "loss": 1.9578,
      "step": 53
    },
    {
      "epoch": 0.972972972972973,
      "grad_norm": 0.4028688073158264,
      "learning_rate": 7.990654205607478e-05,
      "loss": 1.9757,
      "step": 54
    },
    {
      "epoch": 0.990990990990991,
      "grad_norm": 0.3288959264755249,
      "learning_rate": 7.94392523364486e-05,
      "loss": 2.0264,
      "step": 55
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.5668284296989441,
      "learning_rate": 7.897196261682243e-05,
      "loss": 1.7658,
      "step": 56
    },
    {
      "epoch": 1.018018018018018,
      "grad_norm": 0.33850768208503723,
      "learning_rate": 7.850467289719626e-05,
      "loss": 1.9629,
      "step": 57
    },
    {
      "epoch": 1.0360360360360361,
      "grad_norm": 0.37082794308662415,
      "learning_rate": 7.80373831775701e-05,
      "loss": 1.9532,
      "step": 58
    },
    {
      "epoch": 1.054054054054054,
      "grad_norm": 0.3296063542366028,
      "learning_rate": 7.757009345794393e-05,
      "loss": 2.1111,
      "step": 59
    },
    {
      "epoch": 1.072072072072072,
      "grad_norm": 0.40406420826911926,
      "learning_rate": 7.710280373831776e-05,
      "loss": 1.9123,
      "step": 60
    },
    {
      "epoch": 1.09009009009009,
      "grad_norm": 0.3449215888977051,
      "learning_rate": 7.663551401869158e-05,
      "loss": 2.0006,
      "step": 61
    },
    {
      "epoch": 1.1081081081081081,
      "grad_norm": 0.38625335693359375,
      "learning_rate": 7.616822429906543e-05,
      "loss": 1.9342,
      "step": 62
    },
    {
      "epoch": 1.1261261261261262,
      "grad_norm": 0.44683486223220825,
      "learning_rate": 7.570093457943925e-05,
      "loss": 1.9252,
      "step": 63
    },
    {
      "epoch": 1.1441441441441442,
      "grad_norm": 0.4057907164096832,
      "learning_rate": 7.52336448598131e-05,
      "loss": 1.9114,
      "step": 64
    },
    {
      "epoch": 1.1621621621621623,
      "grad_norm": 0.37141749262809753,
      "learning_rate": 7.476635514018692e-05,
      "loss": 2.0312,
      "step": 65
    },
    {
      "epoch": 1.1801801801801801,
      "grad_norm": 0.9156906008720398,
      "learning_rate": 7.429906542056075e-05,
      "loss": 1.9237,
      "step": 66
    },
    {
      "epoch": 1.1981981981981982,
      "grad_norm": 0.3763550817966461,
      "learning_rate": 7.383177570093458e-05,
      "loss": 1.9404,
      "step": 67
    },
    {
      "epoch": 1.2162162162162162,
      "grad_norm": 0.4193122982978821,
      "learning_rate": 7.336448598130842e-05,
      "loss": 2.1411,
      "step": 68
    },
    {
      "epoch": 1.2342342342342343,
      "grad_norm": 0.3509197533130646,
      "learning_rate": 7.289719626168224e-05,
      "loss": 2.0753,
      "step": 69
    },
    {
      "epoch": 1.2522522522522523,
      "grad_norm": 0.38373860716819763,
      "learning_rate": 7.242990654205608e-05,
      "loss": 2.0201,
      "step": 70
    },
    {
      "epoch": 1.2702702702702702,
      "grad_norm": 0.3973683714866638,
      "learning_rate": 7.196261682242991e-05,
      "loss": 2.0227,
      "step": 71
    },
    {
      "epoch": 1.2882882882882882,
      "grad_norm": 0.3776337504386902,
      "learning_rate": 7.149532710280374e-05,
      "loss": 1.9977,
      "step": 72
    },
    {
      "epoch": 1.3063063063063063,
      "grad_norm": 0.3609338402748108,
      "learning_rate": 7.102803738317757e-05,
      "loss": 1.9583,
      "step": 73
    },
    {
      "epoch": 1.3243243243243243,
      "grad_norm": 0.5609249472618103,
      "learning_rate": 7.05607476635514e-05,
      "loss": 1.9526,
      "step": 74
    },
    {
      "epoch": 1.3423423423423424,
      "grad_norm": 0.42948246002197266,
      "learning_rate": 7.009345794392523e-05,
      "loss": 1.8766,
      "step": 75
    },
    {
      "epoch": 1.3603603603603602,
      "grad_norm": 0.4426027834415436,
      "learning_rate": 6.962616822429907e-05,
      "loss": 1.8979,
      "step": 76
    },
    {
      "epoch": 1.3783783783783785,
      "grad_norm": 0.418075829744339,
      "learning_rate": 6.91588785046729e-05,
      "loss": 1.9817,
      "step": 77
    },
    {
      "epoch": 1.3963963963963963,
      "grad_norm": 0.43858182430267334,
      "learning_rate": 6.869158878504673e-05,
      "loss": 1.9727,
      "step": 78
    },
    {
      "epoch": 1.4144144144144144,
      "grad_norm": 0.49554213881492615,
      "learning_rate": 6.822429906542056e-05,
      "loss": 1.8694,
      "step": 79
    },
    {
      "epoch": 1.4324324324324325,
      "grad_norm": 0.44586044549942017,
      "learning_rate": 6.77570093457944e-05,
      "loss": 1.9462,
      "step": 80
    },
    {
      "epoch": 1.4504504504504505,
      "grad_norm": 0.3905578851699829,
      "learning_rate": 6.728971962616822e-05,
      "loss": 2.0499,
      "step": 81
    },
    {
      "epoch": 1.4684684684684686,
      "grad_norm": 0.3826083242893219,
      "learning_rate": 6.682242990654207e-05,
      "loss": 2.0778,
      "step": 82
    },
    {
      "epoch": 1.4864864864864864,
      "grad_norm": 0.4426122307777405,
      "learning_rate": 6.635514018691589e-05,
      "loss": 1.9864,
      "step": 83
    },
    {
      "epoch": 1.5045045045045045,
      "grad_norm": 0.3924534022808075,
      "learning_rate": 6.588785046728972e-05,
      "loss": 1.9441,
      "step": 84
    },
    {
      "epoch": 1.5225225225225225,
      "grad_norm": 0.40386757254600525,
      "learning_rate": 6.542056074766355e-05,
      "loss": 1.8993,
      "step": 85
    },
    {
      "epoch": 1.5405405405405406,
      "grad_norm": 0.38018789887428284,
      "learning_rate": 6.495327102803739e-05,
      "loss": 1.9557,
      "step": 86
    },
    {
      "epoch": 1.5585585585585586,
      "grad_norm": 0.4555302858352661,
      "learning_rate": 6.448598130841122e-05,
      "loss": 1.9806,
      "step": 87
    },
    {
      "epoch": 1.5765765765765765,
      "grad_norm": 0.3911033272743225,
      "learning_rate": 6.401869158878506e-05,
      "loss": 2.0851,
      "step": 88
    },
    {
      "epoch": 1.5945945945945947,
      "grad_norm": 0.403144508600235,
      "learning_rate": 6.355140186915888e-05,
      "loss": 1.8929,
      "step": 89
    },
    {
      "epoch": 1.6126126126126126,
      "grad_norm": 0.4180261492729187,
      "learning_rate": 6.308411214953271e-05,
      "loss": 2.0792,
      "step": 90
    },
    {
      "epoch": 1.6306306306306306,
      "grad_norm": 0.44687554240226746,
      "learning_rate": 6.261682242990654e-05,
      "loss": 1.9501,
      "step": 91
    },
    {
      "epoch": 1.6486486486486487,
      "grad_norm": 0.41066157817840576,
      "learning_rate": 6.214953271028038e-05,
      "loss": 1.9348,
      "step": 92
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.43426960706710815,
      "learning_rate": 6.16822429906542e-05,
      "loss": 2.0874,
      "step": 93
    },
    {
      "epoch": 1.6846846846846848,
      "grad_norm": 0.460895299911499,
      "learning_rate": 6.121495327102805e-05,
      "loss": 2.1335,
      "step": 94
    },
    {
      "epoch": 1.7027027027027026,
      "grad_norm": 0.4447052478790283,
      "learning_rate": 6.074766355140187e-05,
      "loss": 1.8426,
      "step": 95
    },
    {
      "epoch": 1.7207207207207207,
      "grad_norm": 0.40529394149780273,
      "learning_rate": 6.028037383177571e-05,
      "loss": 1.8859,
      "step": 96
    },
    {
      "epoch": 1.7387387387387387,
      "grad_norm": 0.423543244600296,
      "learning_rate": 5.981308411214953e-05,
      "loss": 1.9805,
      "step": 97
    },
    {
      "epoch": 1.7567567567567568,
      "grad_norm": 0.46171656250953674,
      "learning_rate": 5.934579439252337e-05,
      "loss": 1.9248,
      "step": 98
    },
    {
      "epoch": 1.7747747747747749,
      "grad_norm": 0.4007021188735962,
      "learning_rate": 5.8878504672897196e-05,
      "loss": 1.8769,
      "step": 99
    },
    {
      "epoch": 1.7927927927927927,
      "grad_norm": 0.4418306350708008,
      "learning_rate": 5.841121495327103e-05,
      "loss": 1.9402,
      "step": 100
    },
    {
      "epoch": 1.810810810810811,
      "grad_norm": 0.414878785610199,
      "learning_rate": 5.794392523364486e-05,
      "loss": 2.0908,
      "step": 101
    },
    {
      "epoch": 1.8288288288288288,
      "grad_norm": 0.4729587733745575,
      "learning_rate": 5.74766355140187e-05,
      "loss": 1.9747,
      "step": 102
    },
    {
      "epoch": 1.8468468468468469,
      "grad_norm": 0.43790826201438904,
      "learning_rate": 5.700934579439252e-05,
      "loss": 2.0403,
      "step": 103
    },
    {
      "epoch": 1.864864864864865,
      "grad_norm": 0.4547712504863739,
      "learning_rate": 5.654205607476636e-05,
      "loss": 2.0196,
      "step": 104
    },
    {
      "epoch": 1.8828828828828827,
      "grad_norm": 0.4371013939380646,
      "learning_rate": 5.607476635514019e-05,
      "loss": 1.9817,
      "step": 105
    },
    {
      "epoch": 1.900900900900901,
      "grad_norm": 0.4462597668170929,
      "learning_rate": 5.560747663551402e-05,
      "loss": 2.0635,
      "step": 106
    },
    {
      "epoch": 1.9189189189189189,
      "grad_norm": 0.47100308537483215,
      "learning_rate": 5.514018691588785e-05,
      "loss": 1.864,
      "step": 107
    },
    {
      "epoch": 1.936936936936937,
      "grad_norm": 0.46747419238090515,
      "learning_rate": 5.467289719626168e-05,
      "loss": 1.8609,
      "step": 108
    },
    {
      "epoch": 1.954954954954955,
      "grad_norm": 0.41947412490844727,
      "learning_rate": 5.420560747663551e-05,
      "loss": 2.0299,
      "step": 109
    },
    {
      "epoch": 1.972972972972973,
      "grad_norm": 0.45396703481674194,
      "learning_rate": 5.373831775700935e-05,
      "loss": 2.0217,
      "step": 110
    },
    {
      "epoch": 1.990990990990991,
      "grad_norm": 0.4548156261444092,
      "learning_rate": 5.327102803738318e-05,
      "loss": 1.8321,
      "step": 111
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.6119450330734253,
      "learning_rate": 5.280373831775701e-05,
      "loss": 2.1434,
      "step": 112
    },
    {
      "epoch": 2.018018018018018,
      "grad_norm": 0.4478161334991455,
      "learning_rate": 5.233644859813084e-05,
      "loss": 1.9963,
      "step": 113
    },
    {
      "epoch": 2.036036036036036,
      "grad_norm": 0.4102029502391815,
      "learning_rate": 5.186915887850467e-05,
      "loss": 2.1191,
      "step": 114
    },
    {
      "epoch": 2.054054054054054,
      "grad_norm": 0.4623701572418213,
      "learning_rate": 5.14018691588785e-05,
      "loss": 1.9247,
      "step": 115
    },
    {
      "epoch": 2.0720720720720722,
      "grad_norm": 0.4582693874835968,
      "learning_rate": 5.093457943925234e-05,
      "loss": 1.9106,
      "step": 116
    },
    {
      "epoch": 2.09009009009009,
      "grad_norm": 0.42459675669670105,
      "learning_rate": 5.046728971962617e-05,
      "loss": 1.8422,
      "step": 117
    },
    {
      "epoch": 2.108108108108108,
      "grad_norm": 0.5403639078140259,
      "learning_rate": 5e-05,
      "loss": 1.8748,
      "step": 118
    },
    {
      "epoch": 2.126126126126126,
      "grad_norm": 0.4468192756175995,
      "learning_rate": 4.9532710280373836e-05,
      "loss": 1.9561,
      "step": 119
    },
    {
      "epoch": 2.144144144144144,
      "grad_norm": 0.47144901752471924,
      "learning_rate": 4.9065420560747664e-05,
      "loss": 2.0273,
      "step": 120
    },
    {
      "epoch": 2.1621621621621623,
      "grad_norm": 0.4420640766620636,
      "learning_rate": 4.85981308411215e-05,
      "loss": 1.958,
      "step": 121
    },
    {
      "epoch": 2.18018018018018,
      "grad_norm": 0.49685510993003845,
      "learning_rate": 4.813084112149533e-05,
      "loss": 1.7784,
      "step": 122
    },
    {
      "epoch": 2.1981981981981984,
      "grad_norm": 0.46936336159706116,
      "learning_rate": 4.766355140186916e-05,
      "loss": 2.075,
      "step": 123
    },
    {
      "epoch": 2.2162162162162162,
      "grad_norm": 0.46227866411209106,
      "learning_rate": 4.719626168224299e-05,
      "loss": 1.8154,
      "step": 124
    },
    {
      "epoch": 2.234234234234234,
      "grad_norm": 0.4620981514453888,
      "learning_rate": 4.672897196261683e-05,
      "loss": 1.8354,
      "step": 125
    },
    {
      "epoch": 2.2522522522522523,
      "grad_norm": 0.4582028090953827,
      "learning_rate": 4.6261682242990654e-05,
      "loss": 1.8893,
      "step": 126
    },
    {
      "epoch": 2.27027027027027,
      "grad_norm": 0.5313214063644409,
      "learning_rate": 4.579439252336449e-05,
      "loss": 1.8833,
      "step": 127
    },
    {
      "epoch": 2.2882882882882885,
      "grad_norm": 0.50525963306427,
      "learning_rate": 4.532710280373832e-05,
      "loss": 2.0596,
      "step": 128
    },
    {
      "epoch": 2.3063063063063063,
      "grad_norm": 0.4633985757827759,
      "learning_rate": 4.485981308411215e-05,
      "loss": 1.7964,
      "step": 129
    },
    {
      "epoch": 2.3243243243243246,
      "grad_norm": 0.49121153354644775,
      "learning_rate": 4.4392523364485984e-05,
      "loss": 2.1319,
      "step": 130
    },
    {
      "epoch": 2.3423423423423424,
      "grad_norm": 0.5397548079490662,
      "learning_rate": 4.392523364485982e-05,
      "loss": 1.9324,
      "step": 131
    },
    {
      "epoch": 2.3603603603603602,
      "grad_norm": 0.5176860690116882,
      "learning_rate": 4.3457943925233645e-05,
      "loss": 1.8676,
      "step": 132
    },
    {
      "epoch": 2.3783783783783785,
      "grad_norm": 0.48348334431648254,
      "learning_rate": 4.299065420560748e-05,
      "loss": 1.8819,
      "step": 133
    },
    {
      "epoch": 2.3963963963963963,
      "grad_norm": 0.5273210406303406,
      "learning_rate": 4.2523364485981306e-05,
      "loss": 1.9081,
      "step": 134
    },
    {
      "epoch": 2.4144144144144146,
      "grad_norm": 0.45667460560798645,
      "learning_rate": 4.205607476635514e-05,
      "loss": 1.7657,
      "step": 135
    },
    {
      "epoch": 2.4324324324324325,
      "grad_norm": 0.5319989919662476,
      "learning_rate": 4.1588785046728974e-05,
      "loss": 1.848,
      "step": 136
    },
    {
      "epoch": 2.4504504504504503,
      "grad_norm": 0.49191713333129883,
      "learning_rate": 4.11214953271028e-05,
      "loss": 2.0226,
      "step": 137
    },
    {
      "epoch": 2.4684684684684686,
      "grad_norm": 0.541825532913208,
      "learning_rate": 4.0654205607476636e-05,
      "loss": 1.8773,
      "step": 138
    },
    {
      "epoch": 2.4864864864864864,
      "grad_norm": 0.5095556974411011,
      "learning_rate": 4.018691588785047e-05,
      "loss": 1.6705,
      "step": 139
    },
    {
      "epoch": 2.5045045045045047,
      "grad_norm": 0.5048454999923706,
      "learning_rate": 3.97196261682243e-05,
      "loss": 1.9698,
      "step": 140
    },
    {
      "epoch": 2.5225225225225225,
      "grad_norm": 0.5198349952697754,
      "learning_rate": 3.925233644859813e-05,
      "loss": 1.7228,
      "step": 141
    },
    {
      "epoch": 2.5405405405405403,
      "grad_norm": 0.5512752532958984,
      "learning_rate": 3.8785046728971965e-05,
      "loss": 1.9015,
      "step": 142
    },
    {
      "epoch": 2.5585585585585586,
      "grad_norm": 0.4740159809589386,
      "learning_rate": 3.831775700934579e-05,
      "loss": 1.9141,
      "step": 143
    },
    {
      "epoch": 2.5765765765765765,
      "grad_norm": 0.5220773220062256,
      "learning_rate": 3.7850467289719626e-05,
      "loss": 2.0269,
      "step": 144
    },
    {
      "epoch": 2.5945945945945947,
      "grad_norm": 0.5436996817588806,
      "learning_rate": 3.738317757009346e-05,
      "loss": 1.8783,
      "step": 145
    },
    {
      "epoch": 2.6126126126126126,
      "grad_norm": 0.5596913695335388,
      "learning_rate": 3.691588785046729e-05,
      "loss": 1.7679,
      "step": 146
    },
    {
      "epoch": 2.6306306306306304,
      "grad_norm": 0.47611284255981445,
      "learning_rate": 3.644859813084112e-05,
      "loss": 2.0997,
      "step": 147
    },
    {
      "epoch": 2.6486486486486487,
      "grad_norm": 0.5091152191162109,
      "learning_rate": 3.5981308411214956e-05,
      "loss": 1.9663,
      "step": 148
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.5461912751197815,
      "learning_rate": 3.551401869158878e-05,
      "loss": 1.8141,
      "step": 149
    },
    {
      "epoch": 2.684684684684685,
      "grad_norm": 0.5360377430915833,
      "learning_rate": 3.504672897196262e-05,
      "loss": 2.0646,
      "step": 150
    },
    {
      "epoch": 2.7027027027027026,
      "grad_norm": 0.5956213474273682,
      "learning_rate": 3.457943925233645e-05,
      "loss": 1.9658,
      "step": 151
    },
    {
      "epoch": 2.7207207207207205,
      "grad_norm": 0.5783124566078186,
      "learning_rate": 3.411214953271028e-05,
      "loss": 1.8103,
      "step": 152
    },
    {
      "epoch": 2.7387387387387387,
      "grad_norm": 0.499356746673584,
      "learning_rate": 3.364485981308411e-05,
      "loss": 2.0536,
      "step": 153
    },
    {
      "epoch": 2.756756756756757,
      "grad_norm": 0.5087437629699707,
      "learning_rate": 3.3177570093457946e-05,
      "loss": 1.9908,
      "step": 154
    },
    {
      "epoch": 2.774774774774775,
      "grad_norm": 0.5279490351676941,
      "learning_rate": 3.2710280373831774e-05,
      "loss": 1.9824,
      "step": 155
    },
    {
      "epoch": 2.7927927927927927,
      "grad_norm": 0.4978340268135071,
      "learning_rate": 3.224299065420561e-05,
      "loss": 1.8747,
      "step": 156
    },
    {
      "epoch": 2.810810810810811,
      "grad_norm": 0.4736156761646271,
      "learning_rate": 3.177570093457944e-05,
      "loss": 1.9866,
      "step": 157
    },
    {
      "epoch": 2.828828828828829,
      "grad_norm": 0.5608898401260376,
      "learning_rate": 3.130841121495327e-05,
      "loss": 2.0299,
      "step": 158
    },
    {
      "epoch": 2.846846846846847,
      "grad_norm": 0.4918142259120941,
      "learning_rate": 3.08411214953271e-05,
      "loss": 1.8326,
      "step": 159
    },
    {
      "epoch": 2.864864864864865,
      "grad_norm": 0.6525508761405945,
      "learning_rate": 3.0373831775700934e-05,
      "loss": 1.7015,
      "step": 160
    },
    {
      "epoch": 2.8828828828828827,
      "grad_norm": 0.49375081062316895,
      "learning_rate": 2.9906542056074764e-05,
      "loss": 1.9581,
      "step": 161
    },
    {
      "epoch": 2.900900900900901,
      "grad_norm": 0.5688260197639465,
      "learning_rate": 2.9439252336448598e-05,
      "loss": 2.0287,
      "step": 162
    },
    {
      "epoch": 2.918918918918919,
      "grad_norm": 0.5681878328323364,
      "learning_rate": 2.897196261682243e-05,
      "loss": 1.8748,
      "step": 163
    },
    {
      "epoch": 2.936936936936937,
      "grad_norm": 0.47436875104904175,
      "learning_rate": 2.850467289719626e-05,
      "loss": 1.9343,
      "step": 164
    },
    {
      "epoch": 2.954954954954955,
      "grad_norm": 0.5602327585220337,
      "learning_rate": 2.8037383177570094e-05,
      "loss": 1.915,
      "step": 165
    },
    {
      "epoch": 2.972972972972973,
      "grad_norm": 0.5619616508483887,
      "learning_rate": 2.7570093457943924e-05,
      "loss": 1.8601,
      "step": 166
    },
    {
      "epoch": 2.990990990990991,
      "grad_norm": 0.5471289753913879,
      "learning_rate": 2.7102803738317755e-05,
      "loss": 1.751,
      "step": 167
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.719859778881073,
      "learning_rate": 2.663551401869159e-05,
      "loss": 1.9803,
      "step": 168
    },
    {
      "epoch": 3.018018018018018,
      "grad_norm": 0.5613743662834167,
      "learning_rate": 2.616822429906542e-05,
      "loss": 1.8343,
      "step": 169
    },
    {
      "epoch": 3.036036036036036,
      "grad_norm": 0.5262802243232727,
      "learning_rate": 2.570093457943925e-05,
      "loss": 1.8422,
      "step": 170
    },
    {
      "epoch": 3.054054054054054,
      "grad_norm": 0.49427515268325806,
      "learning_rate": 2.5233644859813084e-05,
      "loss": 1.9836,
      "step": 171
    },
    {
      "epoch": 3.0720720720720722,
      "grad_norm": 0.5322190523147583,
      "learning_rate": 2.4766355140186918e-05,
      "loss": 1.8775,
      "step": 172
    },
    {
      "epoch": 3.09009009009009,
      "grad_norm": 0.5700421929359436,
      "learning_rate": 2.429906542056075e-05,
      "loss": 1.7711,
      "step": 173
    },
    {
      "epoch": 3.108108108108108,
      "grad_norm": 0.6416833996772766,
      "learning_rate": 2.383177570093458e-05,
      "loss": 1.8406,
      "step": 174
    },
    {
      "epoch": 3.126126126126126,
      "grad_norm": 0.7611070871353149,
      "learning_rate": 2.3364485981308414e-05,
      "loss": 1.8889,
      "step": 175
    },
    {
      "epoch": 3.144144144144144,
      "grad_norm": 0.5556368231773376,
      "learning_rate": 2.2897196261682244e-05,
      "loss": 1.8651,
      "step": 176
    },
    {
      "epoch": 3.1621621621621623,
      "grad_norm": 0.5847334265708923,
      "learning_rate": 2.2429906542056075e-05,
      "loss": 1.857,
      "step": 177
    },
    {
      "epoch": 3.18018018018018,
      "grad_norm": 0.6334860920906067,
      "learning_rate": 2.196261682242991e-05,
      "loss": 1.8072,
      "step": 178
    },
    {
      "epoch": 3.1981981981981984,
      "grad_norm": 0.5649861693382263,
      "learning_rate": 2.149532710280374e-05,
      "loss": 1.8184,
      "step": 179
    },
    {
      "epoch": 3.2162162162162162,
      "grad_norm": 0.5666905045509338,
      "learning_rate": 2.102803738317757e-05,
      "loss": 1.8383,
      "step": 180
    },
    {
      "epoch": 3.234234234234234,
      "grad_norm": 0.5775569081306458,
      "learning_rate": 2.05607476635514e-05,
      "loss": 1.7859,
      "step": 181
    },
    {
      "epoch": 3.2522522522522523,
      "grad_norm": 0.5734888315200806,
      "learning_rate": 2.0093457943925235e-05,
      "loss": 1.8956,
      "step": 182
    },
    {
      "epoch": 3.27027027027027,
      "grad_norm": 0.599921703338623,
      "learning_rate": 1.9626168224299065e-05,
      "loss": 1.9684,
      "step": 183
    },
    {
      "epoch": 3.2882882882882885,
      "grad_norm": 0.5650666356086731,
      "learning_rate": 1.9158878504672896e-05,
      "loss": 1.8964,
      "step": 184
    },
    {
      "epoch": 3.3063063063063063,
      "grad_norm": 0.5115986466407776,
      "learning_rate": 1.869158878504673e-05,
      "loss": 1.8641,
      "step": 185
    },
    {
      "epoch": 3.3243243243243246,
      "grad_norm": 0.5156318545341492,
      "learning_rate": 1.822429906542056e-05,
      "loss": 1.9428,
      "step": 186
    },
    {
      "epoch": 3.3423423423423424,
      "grad_norm": 0.47419023513793945,
      "learning_rate": 1.775700934579439e-05,
      "loss": 2.0462,
      "step": 187
    },
    {
      "epoch": 3.3603603603603602,
      "grad_norm": 0.6033126711845398,
      "learning_rate": 1.7289719626168225e-05,
      "loss": 1.8526,
      "step": 188
    },
    {
      "epoch": 3.3783783783783785,
      "grad_norm": 0.5753739476203918,
      "learning_rate": 1.6822429906542056e-05,
      "loss": 1.8728,
      "step": 189
    },
    {
      "epoch": 3.3963963963963963,
      "grad_norm": 0.538084089756012,
      "learning_rate": 1.6355140186915887e-05,
      "loss": 1.9722,
      "step": 190
    },
    {
      "epoch": 3.4144144144144146,
      "grad_norm": 0.5149277448654175,
      "learning_rate": 1.588785046728972e-05,
      "loss": 1.7974,
      "step": 191
    },
    {
      "epoch": 3.4324324324324325,
      "grad_norm": 0.5796560049057007,
      "learning_rate": 1.542056074766355e-05,
      "loss": 1.8872,
      "step": 192
    },
    {
      "epoch": 3.4504504504504503,
      "grad_norm": 0.6038906574249268,
      "learning_rate": 1.4953271028037382e-05,
      "loss": 1.8113,
      "step": 193
    },
    {
      "epoch": 3.4684684684684686,
      "grad_norm": 0.5563703179359436,
      "learning_rate": 1.4485981308411214e-05,
      "loss": 1.8449,
      "step": 194
    },
    {
      "epoch": 3.4864864864864864,
      "grad_norm": 0.5663564205169678,
      "learning_rate": 1.4018691588785047e-05,
      "loss": 2.0599,
      "step": 195
    },
    {
      "epoch": 3.5045045045045047,
      "grad_norm": 0.5065940022468567,
      "learning_rate": 1.3551401869158877e-05,
      "loss": 1.8862,
      "step": 196
    },
    {
      "epoch": 3.5225225225225225,
      "grad_norm": 0.5620943307876587,
      "learning_rate": 1.308411214953271e-05,
      "loss": 1.8345,
      "step": 197
    },
    {
      "epoch": 3.5405405405405403,
      "grad_norm": 0.5769988894462585,
      "learning_rate": 1.2616822429906542e-05,
      "loss": 1.874,
      "step": 198
    },
    {
      "epoch": 3.5585585585585586,
      "grad_norm": 0.48588284850120544,
      "learning_rate": 1.2149532710280374e-05,
      "loss": 1.9816,
      "step": 199
    },
    {
      "epoch": 3.5765765765765765,
      "grad_norm": 0.6084792613983154,
      "learning_rate": 1.1682242990654207e-05,
      "loss": 1.9623,
      "step": 200
    },
    {
      "epoch": 3.5945945945945947,
      "grad_norm": 0.6557165384292603,
      "learning_rate": 1.1214953271028037e-05,
      "loss": 1.7232,
      "step": 201
    },
    {
      "epoch": 3.6126126126126126,
      "grad_norm": 0.525191605091095,
      "learning_rate": 1.074766355140187e-05,
      "loss": 2.0603,
      "step": 202
    },
    {
      "epoch": 3.6306306306306304,
      "grad_norm": 0.6658083200454712,
      "learning_rate": 1.02803738317757e-05,
      "loss": 1.7674,
      "step": 203
    },
    {
      "epoch": 3.6486486486486487,
      "grad_norm": 0.5674922466278076,
      "learning_rate": 9.813084112149533e-06,
      "loss": 1.7556,
      "step": 204
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 0.5325369834899902,
      "learning_rate": 9.345794392523365e-06,
      "loss": 1.8565,
      "step": 205
    },
    {
      "epoch": 3.684684684684685,
      "grad_norm": 0.5481227040290833,
      "learning_rate": 8.878504672897196e-06,
      "loss": 2.0007,
      "step": 206
    },
    {
      "epoch": 3.7027027027027026,
      "grad_norm": 0.6242187023162842,
      "learning_rate": 8.411214953271028e-06,
      "loss": 1.8722,
      "step": 207
    },
    {
      "epoch": 3.7207207207207205,
      "grad_norm": 0.5783210396766663,
      "learning_rate": 7.94392523364486e-06,
      "loss": 1.8727,
      "step": 208
    },
    {
      "epoch": 3.7387387387387387,
      "grad_norm": 0.5417590141296387,
      "learning_rate": 7.476635514018691e-06,
      "loss": 1.9655,
      "step": 209
    },
    {
      "epoch": 3.756756756756757,
      "grad_norm": 0.6030580997467041,
      "learning_rate": 7.009345794392523e-06,
      "loss": 1.9594,
      "step": 210
    },
    {
      "epoch": 3.774774774774775,
      "grad_norm": 0.5574659109115601,
      "learning_rate": 6.542056074766355e-06,
      "loss": 1.7901,
      "step": 211
    },
    {
      "epoch": 3.7927927927927927,
      "grad_norm": 0.5498660206794739,
      "learning_rate": 6.074766355140187e-06,
      "loss": 1.8874,
      "step": 212
    },
    {
      "epoch": 3.810810810810811,
      "grad_norm": 0.6425614356994629,
      "learning_rate": 5.607476635514019e-06,
      "loss": 1.931,
      "step": 213
    },
    {
      "epoch": 3.828828828828829,
      "grad_norm": 0.5976663827896118,
      "learning_rate": 5.14018691588785e-06,
      "loss": 1.8196,
      "step": 214
    },
    {
      "epoch": 3.846846846846847,
      "grad_norm": 0.5219494104385376,
      "learning_rate": 4.6728971962616825e-06,
      "loss": 1.7884,
      "step": 215
    },
    {
      "epoch": 3.864864864864865,
      "grad_norm": 0.5473858118057251,
      "learning_rate": 4.205607476635514e-06,
      "loss": 1.8318,
      "step": 216
    },
    {
      "epoch": 3.8828828828828827,
      "grad_norm": 0.56324702501297,
      "learning_rate": 3.7383177570093455e-06,
      "loss": 1.8947,
      "step": 217
    },
    {
      "epoch": 3.900900900900901,
      "grad_norm": 0.5913235545158386,
      "learning_rate": 3.2710280373831774e-06,
      "loss": 2.0177,
      "step": 218
    },
    {
      "epoch": 3.918918918918919,
      "grad_norm": 0.5340273380279541,
      "learning_rate": 2.8037383177570094e-06,
      "loss": 1.8543,
      "step": 219
    },
    {
      "epoch": 3.936936936936937,
      "grad_norm": 0.630746603012085,
      "learning_rate": 2.3364485981308413e-06,
      "loss": 1.9184,
      "step": 220
    },
    {
      "epoch": 3.954954954954955,
      "grad_norm": 0.6685900092124939,
      "learning_rate": 1.8691588785046728e-06,
      "loss": 1.9246,
      "step": 221
    },
    {
      "epoch": 3.972972972972973,
      "grad_norm": 0.6147052645683289,
      "learning_rate": 1.4018691588785047e-06,
      "loss": 1.8467,
      "step": 222
    },
    {
      "epoch": 3.990990990990991,
      "grad_norm": 0.5708595514297485,
      "learning_rate": 9.345794392523364e-07,
      "loss": 2.0023,
      "step": 223
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.8354111909866333,
      "learning_rate": 4.672897196261682e-07,
      "loss": 1.7154,
      "step": 224
    }
  ],
  "logging_steps": 1,
  "max_steps": 224,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 25,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.6298878204359475e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
