{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68d4835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30de1f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_center_jobs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a2c53d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3455433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_information_from_html(html):\n",
    "    \"\"\"\n",
    "    Extract key job info from a LinkedIn job HTML page:\n",
    "    - Seniority level\n",
    "    - Employment type\n",
    "    - Job function\n",
    "    - Industries\n",
    "    - Publish time\n",
    "    - Num applicants\n",
    "    \"\"\"\n",
    "\n",
    "    results = {\n",
    "        \"Seniority level\": \"\",\n",
    "        \"Employment type\": \"\",\n",
    "        \"Job function\": \"\",\n",
    "        \"Industries\": \"\",\n",
    "        \"publish_time\": \"\",\n",
    "        \"num_applicants\": \"\"\n",
    "    }\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # --- Publish time & applicants ---\n",
    "    company = soup.find('h4', class_='top-card-layout__second-subline')\n",
    "    if company:\n",
    "        spans = company.find_all('span')\n",
    "        if len(spans) >= 3:\n",
    "            results['publish_time'] = spans[2].get_text(strip=True)\n",
    "        if len(spans) >= 4:\n",
    "            results['num_applicants'] = spans[3].get_text(strip=True)\n",
    "\n",
    "    # --- Job description (structured criteria) ---\n",
    "    description = soup.find('section', class_='description')\n",
    "    if description:\n",
    "        job_criteria = description.find('ul', class_='description__job-criteria-list')\n",
    "        if job_criteria:\n",
    "            criteria = job_criteria.find_all('li')\n",
    "            for criterion in criteria:\n",
    "                key = criterion.find('h3')\n",
    "                value = criterion.find('span')\n",
    "                if key and value:\n",
    "                    key_text = key.get_text(strip=True)\n",
    "                    val_text = value.get_text(strip=True)\n",
    "                    if key_text in results:  # only keep the 4 description-related ones\n",
    "                        results[key_text] = val_text\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f52adf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_job_info_from_urls(df, url_col=\"job_url\", sleep_between=1.5, max_retries=3):\n",
    "    extracted_data = []\n",
    "\n",
    "    for i, url in enumerate(df[url_col]):\n",
    "        html = None\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.get(url, headers=header, timeout=15)\n",
    "                if response.status_code == 200:\n",
    "                    html = response.text\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"{response.status_code} @ row {i}\")\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"{e} @ row {i} (attempt {attempt+1})\")\n",
    "            sleep(sleep_between)  # wait before retry\n",
    "\n",
    "        if html:\n",
    "            info = extract_information_from_html(html)\n",
    "            extracted_data.append(info)\n",
    "        else:\n",
    "            extracted_data.append({k: \"\" for k in [\n",
    "                \"Seniority level\", \"Employment type\", \"Job function\", \n",
    "                \"Industries\", \"publish_time\", \"num_applicants\"\n",
    "            ]})\n",
    "\n",
    "        sleep(sleep_between)\n",
    "\n",
    "    info_df = pd.DataFrame(extracted_data)\n",
    "    return pd.concat([df.reset_index(drop=True), info_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d077de9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = extract_job_info_from_urls(df)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4e995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('data_center_jobs_enriched.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
