{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "080ad382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, random, re, urllib.parse\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from getpass import getpass\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d89b9929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Login successful!\n"
     ]
    }
   ],
   "source": [
    "USERNAME = \"zh272@georgetown.edu\"\n",
    "PASSWORD = getpass(\"üîê Enter LinkedIn Password: \")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Go to LinkedIn login page\n",
    "driver.get(\"https://www.linkedin.com/login\")\n",
    "time.sleep(2)\n",
    "\n",
    "# Enter email and password\n",
    "driver.find_element(By.ID, \"username\").send_keys(USERNAME)\n",
    "driver.find_element(By.ID, \"password\").send_keys(PASSWORD + Keys.RETURN)\n",
    "time.sleep(5)\n",
    "\n",
    "# Verify login\n",
    "if \"feed\" in driver.current_url or \"jobs\" in driver.current_url:\n",
    "    print(\"‚úÖ Login successful!\")\n",
    "else:\n",
    "    print(\"‚ùå Login may have failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "00bbeced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 270 combinations...\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "experience_levels = {\n",
    "\"Internship\": \"1\", \"Entry level\": \"2\", \"Associate\": \"3\",\n",
    "\"Mid-Senior\": \"4\", \"Director\": \"5\", \"Executive\": \"6\"\n",
    "}\n",
    "\n",
    "\n",
    "remote_types = {\n",
    "\"On-site\": \"1\", \"Hybrid\": \"2\", \"Remote\": \"3\"\n",
    "}\n",
    "\n",
    "\n",
    "date_posted = {\n",
    "\"Any time\": None,\"Past week\": \"r604800\", \"Past month\": \"r2592000\"\n",
    "}\n",
    "\n",
    "\n",
    "salary_ranges = {\n",
    "\"$40K+\": \"1\", \"$60K+\": \"2\", \"$80K+\": \"3\", \"$100K+\": \"4\", \"$120K+\": \"5\"\n",
    "}\n",
    "\n",
    "\n",
    "# --- BASE URL ---\n",
    "base_url = \"https://www.linkedin.com/jobs/search/?\"\n",
    "\n",
    "\n",
    "# --- FIXED PARAMETERS ---\n",
    "fixed_params = {\n",
    "\"keywords\": \"data center\",\n",
    "\"location\": \"United States\",\n",
    "\"sortBy\": \"R\"\n",
    "}\n",
    "\n",
    "combinations = list(itertools.product(\n",
    "experience_levels.items(),\n",
    "remote_types.items(),\n",
    "date_posted.items(),\n",
    "salary_ranges.items()\n",
    "))\n",
    "\n",
    "print(f\"Generating {len(combinations)} combinations...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2ec5d76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated 270 URLs.\n"
     ]
    }
   ],
   "source": [
    "import urllib.parse\n",
    "\n",
    "generated_urls = []\n",
    "\n",
    "for exp, remote, date, salary in combinations:\n",
    "    exp_label, exp_val = exp\n",
    "    remote_label, remote_val = remote\n",
    "    date_label, date_val = date\n",
    "    salary_label, salary_val = salary\n",
    "    query_params = fixed_params.copy()\n",
    "    query_params.update({\n",
    "        \"f_E\": exp_val,\n",
    "        \"f_WT\": remote_val,\n",
    "        \"f_TPR\": date_val,\n",
    "        \"f_SB2\": salary_val\n",
    "    })\n",
    "    final_url = base_url + urllib.parse.urlencode(query_params)\n",
    "    generated_urls.append(final_url)\n",
    "\n",
    "\n",
    "print(f\"Successfully generated {len(generated_urls)} URLs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fe77c0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTAINER_SELECTORS = [\n",
    "    \"ul.scaffold-layout__list-container\",\n",
    "    \"div.jobs-search-results-list__container\",\n",
    "    \"div.jobs-search-two-pane__results\",\n",
    "    \"section.two-pane-serp-page__results-list\",\n",
    "]\n",
    "ITEM_SELECTORS = [\n",
    "    \"li.scaffold-layout__list-item\",\n",
    "    \"li[data-occludable-job-id]\",\n",
    "    \"ul.jobs-search-results__list > li\",\n",
    "    \"div.jobs-search-results-list__list-item\",\n",
    "]\n",
    "\n",
    "def _sleep(a=0.35, b=0.85): time.sleep(random.uniform(a,b))\n",
    "\n",
    "def _first_el(root, selectors, by_css=True, timeout=0):\n",
    "    if timeout:\n",
    "        for sel in selectors:\n",
    "            try:\n",
    "                if by_css:\n",
    "                    return WebDriverWait(root, timeout).until(EC.presence_of_element_located((By.CSS_SELECTOR, sel)))\n",
    "                else:\n",
    "                    return WebDriverWait(root, timeout).until(EC.presence_of_element_located((By.XPATH, sel)))\n",
    "            except TimeoutException:\n",
    "                continue\n",
    "        return None\n",
    "    else:\n",
    "        for sel in selectors:\n",
    "            try:\n",
    "                els = root.find_elements(By.CSS_SELECTOR if by_css else By.XPATH, sel)\n",
    "                if els: return els[0]\n",
    "            except Exception:\n",
    "                pass\n",
    "        return None\n",
    "\n",
    "def _first_text(root, selectors):\n",
    "    el = _first_el(root, selectors, by_css=True, timeout=0)\n",
    "    try: return (el.text or \"\").strip()\n",
    "    except: return \"\"\n",
    "\n",
    "def _first_attr(root, selectors, attr):\n",
    "    el = _first_el(root, selectors, by_css=True, timeout=0)\n",
    "    try: return el.get_attribute(attr) or \"\"\n",
    "    except: return \"\"\n",
    "\n",
    "def wait_results_loaded(driver, timeout=12):\n",
    "    for sel in CONTAINER_SELECTORS:\n",
    "        try:\n",
    "            WebDriverWait(driver, timeout).until(EC.presence_of_element_located((By.CSS_SELECTOR, sel)))\n",
    "            return True\n",
    "        except TimeoutException:\n",
    "            continue\n",
    "    return False\n",
    "\n",
    "def expand_all_in(driver, root=None, max_clicks=8):\n",
    "    scope = root or driver\n",
    "    # Both the old & new ‚ÄúSee more‚Äù buttons\n",
    "    sels = [\n",
    "        \"button.show-more-less-html__button--more\",\n",
    "        \"button.show-more-less-html__button\",\n",
    "        \"button[aria-expanded='false']\",\n",
    "    ]\n",
    "    clicks = 0\n",
    "    while clicks < max_clicks:\n",
    "        btn = _first_el(scope, sels, by_css=True, timeout=0)\n",
    "        if not btn: break\n",
    "        try:\n",
    "            btn.click()\n",
    "        except (ElementClickInterceptedException, StaleElementReferenceException):\n",
    "            try:\n",
    "                driver.execute_script(\"arguments[0].click();\", btn)\n",
    "            except Exception:\n",
    "                break\n",
    "        _sleep(0.2, 0.4)\n",
    "        clicks += 1\n",
    "\n",
    "def get_title_company(driver):\n",
    "    title = \"\"\n",
    "    company = \"\"\n",
    "    try:\n",
    "        title = WebDriverWait(driver, 8).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"h1.top-card-layout__title, h1.topcard__title\"))\n",
    "        ).text.strip()\n",
    "    except TimeoutException:\n",
    "        pass\n",
    "    comp_el = _first_el(driver, [\"a.topcard__org-name-link\", \"span.topcard__flavor\"], timeout=0)\n",
    "    if comp_el:\n",
    "        try: company = comp_el.text.strip()\n",
    "        except: pass\n",
    "    return title, company\n",
    "\n",
    "def get_description_root(driver):\n",
    "    for sel in [\"#job-details\", \".jobs-description__content\", \".jobs-box__html-content\", \".show-more-less-html__markup\"]:\n",
    "        found = driver.find_elements(By.CSS_SELECTOR, sel)\n",
    "        if found: return found[0]\n",
    "    return None\n",
    "\n",
    "def get_salary(driver):\n",
    "    # Heuristic scan near the top card area first, then global\n",
    "    areas = [\n",
    "        \".jobs-unified-top-card\", \".job-view-layout\", \"section.jobs-unified-top-card__primary-description\"\n",
    "    ]\n",
    "    for area in areas:\n",
    "        for el in driver.find_elements(By.CSS_SELECTOR, area+\" *\"):\n",
    "            try:\n",
    "                t = (el.text or \"\").strip()\n",
    "                if \"$\" in t or \"/hr\" in t or \"per year\" in t or \"per hour\" in t or \"salary\" in t.lower():\n",
    "                    return t\n",
    "            except: pass\n",
    "    # fallback\n",
    "    try:\n",
    "        t = driver.find_element(By.XPATH, \"//*[self::span or self::div][contains(text(),'$') or contains(.,'/hr') or contains(.,'per year') or contains(.,'per hour') or contains(translate(., 'SALARY', 'salary'),'salary')]\").text\n",
    "        return t.strip()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "# Split to requested sections\n",
    "SECTION_HEADERS = [\n",
    "    (\"Responsibilities\", [\"responsibilities\",\"duties\",\"what you will do\",\"what you‚Äôll do\"]),\n",
    "    (\"QualificationsRequired\", [\"required qualifications\",\"basic qualifications\",\"minimum qualifications\",\"requirements\"]),\n",
    "    (\"QualificationsPreferred\", [\"preferred qualifications\",\"nice to have\",\"preferred\"]),\n",
    "    (\"Requirements\", [\"requirements\"]),  # catch-all\n",
    "]\n",
    "def split_sections(desc):\n",
    "    out = {\"Description\": desc or \"\", \"Responsibilities\":\"\", \"QualificationsRequired\":\"\", \"QualificationsPreferred\":\"\", \"Requirements\":\"\"}\n",
    "    txt = desc or \"\"\n",
    "    low = txt.lower()\n",
    "    hits = []\n",
    "    for col, keys in SECTION_HEADERS:\n",
    "        for k in keys:\n",
    "            i = low.find(k)\n",
    "            if i != -1:\n",
    "                hits.append((i, col, k))\n",
    "                break\n",
    "    if not hits: return out\n",
    "    hits.sort(key=lambda x: x[0])\n",
    "    for idx, (start, col, key) in enumerate(hits):\n",
    "        end = hits[idx+1][0] if idx+1 < len(hits) else len(txt)\n",
    "        seg = txt[start:end].strip()\n",
    "        seg = re.sub(rf\"(?is)^{re.escape(key)}\\s*:?\\s*\", \"\", seg, count=1)\n",
    "        out[col] = seg.strip()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "856a13e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_search_url_to_rows(driver, url, max_pages=40, per_page_limit=None):\n",
    "    rows = []\n",
    "    for start in range(0, max_pages*25, 25):\n",
    "        # build paginated URL\n",
    "        u = urllib.parse.urlparse(url)\n",
    "        q = dict(urllib.parse.parse_qsl(u.query, keep_blank_values=True))\n",
    "        q[\"start\"] = str(start)\n",
    "        page_url = urllib.parse.urlunparse((u.scheme, u.netloc, u.path, u.params,\n",
    "                                            urllib.parse.urlencode(q, doseq=True), u.fragment))\n",
    "        driver.get(page_url)\n",
    "\n",
    "        if not wait_results_loaded(driver, timeout=12):\n",
    "            break\n",
    "\n",
    "        # find job cards\n",
    "        cards = []\n",
    "        for sel in ITEM_SELECTORS:\n",
    "            cards = driver.find_elements(By.CSS_SELECTOR, sel)\n",
    "            if cards: break\n",
    "        if not cards:\n",
    "            break\n",
    "\n",
    "        list_handle = driver.current_window_handle\n",
    "        taken = 0\n",
    "\n",
    "        for job in cards:\n",
    "            try:\n",
    "                link_el = _first_el(job, [\n",
    "                    \"a.base-card__full-link\", \n",
    "                    \"a.job-card-container__link\", \n",
    "                    \"a[href*='/jobs/view/']\"\n",
    "                ], timeout=0)\n",
    "                if not link_el: \n",
    "                    continue\n",
    "\n",
    "                link = link_el.get_attribute(\"href\") or \"\"\n",
    "                if not link:\n",
    "                    continue\n",
    "\n",
    "                # üü¢ open full job in a new tab\n",
    "                driver.execute_script(\"window.open(arguments[0], '_blank');\", link)\n",
    "                WebDriverWait(driver, 10).until(lambda d: len(d.window_handles) > 1)\n",
    "                driver.switch_to.window(driver.window_handles[-1])\n",
    "\n",
    "                WebDriverWait(driver, 12).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, \".jobs-unified-top-card, h1.top-card-layout__title\"))\n",
    "                )\n",
    "\n",
    "                expand_all_in(driver, None, max_clicks=6)\n",
    "\n",
    "                joburl = (driver.current_url or \"\").split(\"?\")[0]\n",
    "                title, company = get_title_company(driver)\n",
    "                desc_root = get_description_root(driver)\n",
    "                desc = desc_root.text.strip() if desc_root else \"\"\n",
    "                salary = get_salary(driver)\n",
    "\n",
    "                secs = split_sections(desc)\n",
    "\n",
    "                rows.append({\n",
    "                    \"JobTitle\": title,\n",
    "                    \"Company\": company,\n",
    "                    \"JobURL\": joburl,\n",
    "                    \"SearchURL\": url,\n",
    "                    \"Description\": secs.get(\"Description\",\"\"),\n",
    "                    \"Responsibilities\": secs.get(\"Responsibilities\",\"\"),\n",
    "                    \"QualificationsRequired\": secs.get(\"QualificationsRequired\",\"\"),\n",
    "                    \"QualificationsPreferred\": secs.get(\"QualificationsPreferred\",\"\"),\n",
    "                    \"Requirements\": secs.get(\"Requirements\",\"\"),\n",
    "                    \"Salary\": salary,\n",
    "                })\n",
    "\n",
    "                # close tab, return to results\n",
    "                driver.close()\n",
    "                driver.switch_to.window(list_handle)\n",
    "                _sleep()\n",
    "\n",
    "                taken += 1\n",
    "                if per_page_limit and taken >= per_page_limit:\n",
    "                    break\n",
    "\n",
    "            except Exception:\n",
    "                try:\n",
    "                    if driver.current_window_handle != list_handle:\n",
    "                        driver.close()\n",
    "                        driver.switch_to.window(list_handle)\n",
    "                except:\n",
    "                    pass\n",
    "                continue\n",
    "\n",
    "        if len(cards) < 10:\n",
    "            break\n",
    "        _sleep(0.9, 1.6)\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5ea362bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_and_collect(driver, urls, max_pages=40, per_page_limit=None):\n",
    "    \"\"\"\n",
    "    Scrape LinkedIn jobs from a list of search URLs.\n",
    "    Returns a Pandas DataFrame with the results.\n",
    "    (CSV writing removed ‚Äî you can export later.)\n",
    "    \"\"\"\n",
    "    all_rows = []\n",
    "    for u in urls:\n",
    "        rows = scrape_search_url_to_rows(\n",
    "            driver, u, max_pages=max_pages, per_page_limit=per_page_limit\n",
    "        )\n",
    "        all_rows.extend(rows)\n",
    "        _sleep(1.0, 2.0)\n",
    "\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    if \"JobURL\" in df.columns:\n",
    "        df = df.drop_duplicates(subset=[\"JobURL\"], keep=\"first\")\n",
    "    print(f\"Collected {len(df)} rows from {len(urls)} search URLs\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c8b6cd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 0 rows from 1 search URLs\n"
     ]
    }
   ],
   "source": [
    "df = scrape_and_collect(driver, [generated_urls[0]], max_pages=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1cc74b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
